{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a43338f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.create_lang_detector(nlp, name)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import scispacy\n",
    "import spacy\n",
    "import en_core_sci_lg\n",
    "from spacy_langdetect import LanguageDetector\n",
    "from spacy.language import Language\n",
    "\n",
    "def create_lang_detector(nlp, name):\n",
    "    return LanguageDetector()\n",
    "Language.factory('language_detector', func=create_lang_detector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e4a796",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "I plan to preprocess the data using the following steps\n",
    "* Read the metadata.csv, get the whole picture of the publications we have.\n",
    "* Extract the body text from publications in pdf and pmc respectively.\n",
    "* Merge the metadata and the corresponding body text.\n",
    "* Clean the merged data: remove duplicates, remove blank text.\n",
    "* Creating new features: is_covid19 (binary indicator indicating whether the publication is related to covid19), text_language (remove non-english pubs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61eb62e",
   "metadata": {},
   "source": [
    "### Read metadata & rough descriptive analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8b39652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['cord_uid', 'sha', 'source_x', 'title', 'doi', 'pmcid', 'pubmed_id',\n",
      "       'license', 'abstract', 'publish_time', 'authors', 'journal', 'mag_id',\n",
      "       'who_covidence_id', 'arxiv_id', 'pdf_json_files', 'pmc_json_files',\n",
      "       'url', 's2_id'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# read metadata\n",
    "metadata = pd.read_csv('./CovidData/metadata.csv', dtype={\n",
    "    'pubmed_id': str, \n",
    "    'Microsoft Academic Paper ID': str, 'doi': str\n",
    "})\n",
    "# check the data structure\n",
    "print(metadata.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4403fa2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percent of null in each column: \n",
      " cord_uid            0.000000\n",
      "sha                 0.686875\n",
      "source_x            0.000000\n",
      "title               0.000493\n",
      "doi                 0.504897\n",
      "pmcid               0.675089\n",
      "pubmed_id           0.571102\n",
      "license             0.000000\n",
      "abstract            0.261919\n",
      "publish_time        0.000307\n",
      "authors             0.023066\n",
      "journal             0.061969\n",
      "mag_id              1.000000\n",
      "who_covidence_id    0.525287\n",
      "arxiv_id            0.987713\n",
      "pdf_json_files      0.686875\n",
      "pmc_json_files      0.743873\n",
      "url                 0.472467\n",
      "s2_id               0.076933\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# a rough descriptive analysis\n",
    "print('The percent of null in each column: \\n', metadata.isnull().sum()/len(metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1e1745",
   "metadata": {},
   "source": [
    "What we care about are `sha` and `pmcid` which are unique for each publication and are the keys to merge the publications with the metadata. The percentages of null in these two columns are both over 50%, I plan to remove the ones with both the null `sha` and null `pmcid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "740b12ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of unique cord_uid: 659301 \n",
      " the number of unique sha: 231780 \n",
      " the number of unique titles: 554017\n"
     ]
    }
   ],
   "source": [
    "# cord_uid: this persistent identifier is a 8-char alphanumeric string unique to each entry\n",
    "# sha: the hash of the PDFs are in 'sha'\n",
    "# the number of unique titles\n",
    "print('the number of unique cord_uid:', metadata.cord_uid.nunique(), '\\n',\n",
    "      'the number of unique sha:', metadata.sha.nunique(), '\\n',\n",
    "      'the number of unique titles:', metadata.title.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6023511",
   "metadata": {},
   "source": [
    "### Extract the body text from publications in pdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72b61527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"paper_id\": \"c40b7d23a6c005ab11aa812d9d43567140feedeb\",\n",
      "    \"metadata\": {\n",
      "        \"title\": \"Certifying Irreducibility in Z[x]\",\n",
      "        \"authors\": [\n",
      "            {\n",
      "                \"first\": \"John\",\n",
      "                \"middle\": [],\n",
      "                \"last\": \"Abbott\",\n",
      "                \"suffix\": \"\",\n",
      "                \"affiliation\": {\n",
      "                    \"laboratory\": \"\",\n",
      "                    \"institution\": \"Universit\\u00e4t Passau\",\n",
      "                    \"location\": {\n",
      "                        \"settlement\": \"Passau\",\n",
      "                        \"country\": \"Germany\"\n",
      "                    }\n",
      "                },\n",
      "                \"email\": \"abbott@dima.unige.it\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"abstract\": [\n",
      "        {\n",
      "            \"text\": \"We consider the question of certifying that a polynomial in\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Abstract\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"Knowing that a polynomial is irreducible lets us recognise that a quotient ring is actually a field extension (equiv. that a polynomial ideal is maximal). Checking that a polynomial is irreducible by factorizing it is unsatisfactory because it requires trusting a relatively large and complicated program (whose correctness cannot easily be verified). We present a practical method for generating certificates of irreducibility which can be verified by relatively simple computations; we assume that primes and irreducibles in Fp[x] are self-certifying.\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Abstract\"\n",
      "        }\n",
      "    ],\n",
      "    \"body_text\": [\n",
      "        {\n",
      "            \"text\": \"A certificate that object X has property P is a \\\"small\\\" amount of extra information C such that some quick and simple computations with X and C suffice to confirm that X does have the property. We illustrate this vague definition with a well-known, concrete example. Example 1. We can certify that a positive integer n is prime using a Lucas-Pratt certificate [9] . The idea is to find a witness w such that w n\\u22121 \\u2261 1 mod n and w (n\\u22121)/q \\u2261 1 mod n for all prime factors q of n \\u2212 1.\",\n",
      "            \"cite_spans\": [\n",
      "                {\n",
      "                    \"start\": 360,\n",
      "                    \"end\": 363,\n",
      "                    \"text\": \"[9]\",\n",
      "                    \"ref_id\": \"BIBREF10\"\n",
      "                }\n",
      "            ],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"What Is a \\\"Certificate\\\"?\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"These certificates have a recursive structure, since in general we must certify each prime factor q of n \\u2212 1. To avoid infinite recursion we say that all small primes up to some limit are \\\"self-certifying\\\" (i.e. they need no certificate).\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"What Is a \\\"Certificate\\\"?\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"Thus a Lucas-Pratt certificate comprises a witness w, and a list of prime factors q 1 , q 2 , . . . of n \\u2212 1 (and certificates for each q j ). Verification involves:\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"What Is a \\\"Certificate\\\"?\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"-verify that w n\\u22121 \\u2261 1 mod n; -verify that each w (n\\u22121)/qj \\u2261 1 mod n; -verify that n \\u2212 1 = j q ej j for positive exponents e j ; -recursively verify that each q j is prime.\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"What Is a \\\"Certificate\\\"?\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"The operations required to verify such a certificate are: iteration over a list, exponentiation modulo an integer, comparison with 1, division of integers, and divisibility testing of integers. These are all simple operations, and the entire function to verify a Lucas-Pratt certificate is small enough to be fully verifiable itself.\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"What Is a \\\"Certificate\\\"?\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"An important point in this example is that the certificate actually involves several cases: namely, if the prime is small enough, the certificate just says that it is a \\\"small prime\\\" (e.g. we can verify by table-lookup); otherwise the certificate contains a non-trivial body. In this instance there are just two possible cases.\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"What Is a \\\"Certificate\\\"?\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"We note that generating a Lucas-Pratt certificate could be costly because the prime factorization of n \\u2212 1 must be computed.\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"What Is a \\\"Certificate\\\"?\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"The total cost of a certificate comprises several components: -computational cost of generating the certificate; -size of the certificate (e.g. cost of storage or transmission); -computational cost of verification given the certificate; -size and code complexity of the verifier.\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Costs of a Certificate\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"In the case of certifying the irreducibility of a polynomial in Z[x] we could issue trivial certificates for all polynomials, and say that the verifier simply has to be an implementation of a polynomial factorizer. We regard this as unsatisfactory because the size and code complexity of the verifier are too high.\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Costs of a Certificate\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"We can immediately reduce from Q[x] to Z[x] thanks to Gauss's Lemma (for polynomials): let f \\u2208 Q[x] be non-constant then f is irreducible if and only if prim(f ) \\u2208 Z[x] is irreducible, where prim(f ) = \\u03b1f and the uniquely defined, non-zero factor \\u03b1 \\u2208 Q is such that all coefficients of prim(f ) are integers with common factor 1, and the leading coefficient is positive,\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Irreducibility Criteria for Z[x] and Q[x]\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"The problem of certifying irreducibility in Z[x] has a long history, and has already been considered by several people. Here is a list of some approaches: -give a \\\"large\\\" evaluation point n such that f (n) has a large prime factor; -degree analysis (from factorizations over one or more finite fields) 1 ; -a linear polynomial is obviously irreducible; -Newton polygon methods (e.g. Sch\\u00f6nemann, Eisenstein, and Dumas [4] ); -Vahlen-Capelli lemma [10] for binomials -Perron's Criterion [8] ; -the coefficients are (non-negative) digits of a prime to some base b (e.g. [8] ).\",\n",
      "            \"cite_spans\": [\n",
      "                {\n",
      "                    \"start\": 417,\n",
      "                    \"end\": 420,\n",
      "                    \"text\": \"[4]\",\n",
      "                    \"ref_id\": \"BIBREF5\"\n",
      "                },\n",
      "                {\n",
      "                    \"start\": 446,\n",
      "                    \"end\": 450,\n",
      "                    \"text\": \"[10]\",\n",
      "                    \"ref_id\": \"BIBREF11\"\n",
      "                },\n",
      "                {\n",
      "                    \"start\": 485,\n",
      "                    \"end\": 488,\n",
      "                    \"text\": \"[8]\",\n",
      "                    \"ref_id\": \"BIBREF9\"\n",
      "                },\n",
      "                {\n",
      "                    \"start\": 567,\n",
      "                    \"end\": 570,\n",
      "                    \"text\": \"[8]\",\n",
      "                    \"ref_id\": \"BIBREF9\"\n",
      "                }\n",
      "            ],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Irreducibility Criteria for Z[x] and Q[x]\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"The first technique in the list was inspired by ideas from [3] ; it seems to be new.\",\n",
      "            \"cite_spans\": [\n",
      "                {\n",
      "                    \"start\": 59,\n",
      "                    \"end\": 62,\n",
      "                    \"text\": \"[3]\",\n",
      "                    \"ref_id\": \"BIBREF3\"\n",
      "                }\n",
      "            ],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Irreducibility Criteria for Z[x] and Q[x]\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"In this presentation, we shall assume that the degree is at least 2, and shall concentrate on the first two methods as they are far more widely applicable than others listed.\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Irreducibility Criteria for Z[x] and Q[x]\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"Factor degree analysis is a well-known, behind-the-scenes technique in polynomial factorization. It involves using degrees of modular factors to obtain a list of excluded degrees for factors in Z[x].\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Factor Degree Analysis\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"We define a factor degree lower bound for f \\u2208 Z[x] to be \\u0394 \\u2208 N such that we have excluded all degrees less than \\u0394, e.g. through factor degree analysis. We can certify this lower bound by accompanying it with the modular factorizations used. Clearly, if degree analysis excludes all degrees up to 1 2 deg f then we have proved that f is irreducible. Finally, we may always take \\u0394 = 1 without any degree analysis.\",\n",
      "            \"cite_spans\": [\n",
      "                {\n",
      "                    \"start\": 296,\n",
      "                    \"end\": 297,\n",
      "                    \"text\": \"1\",\n",
      "                    \"ref_id\": \"BIBREF0\"\n",
      "                }\n",
      "            ],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Factor Degree Analysis\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"In many cases we can indeed prove/certify irreducibility via degree analysis. However, there are some (infinite) families of polynomials where one must use \\\"larger\\\" primes, and there are also (infinite) families where irreducibility cannot be proved via factor degree analysis (e.g. resultants, in particular Swinnerton-Dyer polynomials, see also [6] ).\",\n",
      "            \"cite_spans\": [\n",
      "                {\n",
      "                    \"start\": 347,\n",
      "                    \"end\": 350,\n",
      "                    \"text\": \"[6]\",\n",
      "                    \"ref_id\": \"BIBREF7\"\n",
      "                }\n",
      "            ],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Factor Degree Analysis\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"Example 2. The well-known, classical example of a polynomial which cannot be proved irreducible by degree analysis is x 4 + 1: every modular factorization is into either 4 linears or 2 quadratics, so this does not let us exclude the possible existence of a degree 2 factor.\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Factor Degree Analysis\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"There are also many polynomials which can be proved irreducible by degree analysis, but are not irreducible modulo any prime; this property depends on the Galois group of the polynomial. For instance, f = x 4 + x 3 + 3x + 4 is one such polynomial: modulo 2 the irreducible factors have degrees 1 and 3, and modulo 5 both factors have degree 2; but it is never irreducible modulo p.\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Factor Degree Analysis\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"Degree Analysis Certificate. A degree analysis certificate comprises -a subset D \\u2286 {1, 2, . . . , 1 2 deg f } of \\\"not excluded\\\" factor degrees -a list, L, of pairs: a prime p, and the irreducible factors of f modulo p If D = \\u2205, we have a certificate of ireducibility; otherwise the smallest element of the set is a factor degree lower bound.\",\n",
      "            \"cite_spans\": [\n",
      "                {\n",
      "                    \"start\": 98,\n",
      "                    \"end\": 99,\n",
      "                    \"text\": \"1\",\n",
      "                    \"ref_id\": \"BIBREF0\"\n",
      "                }\n",
      "            ],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Factor Degree Analysis\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"Verification of the certificate involves the following steps:\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Factor Degree Analysis\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"-for each entry in L, check that the product of the modular factors is f ; -for each entry in L, compute the set of degrees of all possible products of the modular factors; verify that their intersection is D; -check that each modular factor is irreducible (e.g. use gaussian reduction to compute the rank of B \\u2212 I where B is the Berlekamp matrix).\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Factor Degree Analysis\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"The main cost of the verification is the computation of B and the rank of B \\u2212 I; the cost of computing B is greater for larger primes, so we prefer to generate certificates which use smaller primes if possible.\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Factor Degree Analysis\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"We would like to know, in practice, how costly it is to produce a useful degree analysis certificate, and how large the resulting certificate could be. More specifically:\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Practical Matters.\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"-How many different primes should we consider? And how large? -How to find a minimal set of primes yielding the factor degree subset? -How many primes are typically in the minimal set?\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Practical Matters.\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"In our experience, a minimal length list very rarely contains more than 3 entries, but we should expect to consider many more primes during generation of the certificate. We can construct irreducible polynomials which require considering \\\"large\\\" primes to obtain useful degree information (e.g. x 2 + Nx + N where N = 1000000!) but in many cases \\\"small\\\" primes up to around deg f suffice.\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Practical Matters.\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"Bunyakowski's conjecture (e.g. see page 323 of [7] ) states that if f \\u2208 Z[x] is irreducible (and has trivial fixed divisor) then |f (n)| is prime for infinitely many n \\u2208 Z. Assuming the conjecture is true, we can get a certificate of irreducibility by finding a suitable evaluation point n (and perhaps including a certificate that |f (n)| is prime). Applying Bunyakowski's conjecture directly is inconvenient for two reasons:\",\n",
      "            \"cite_spans\": [\n",
      "                {\n",
      "                    \"start\": 47,\n",
      "                    \"end\": 50,\n",
      "                    \"text\": \"[7]\",\n",
      "                    \"ref_id\": \"BIBREF8\"\n",
      "                }\n",
      "            ],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Irreducibility Certificates for Z[x] via Evaluation\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"-we want to handle polynomials with non-trivial fixed divisor; -finding a suitable n may be costly, and the resulting |f (n)| may be large.\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Irreducibility Certificates for Z[x] via Evaluation\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"The first point is solved by an easy generalization of the conjecture: let f \\u2208 Z[x] be irreducible and \\u03b4 be its fixed divisor, then there are infinitely many n \\u2208 Z such that |f (n)|/\\u03b4 is prime. The second point is a genuine inconvenience: for some polynomials, it can be costly to find a \\\"Bunyakowski prime,\\\" and the prime itself will be large (and thus costly to verify). For example, let f = x 16 +4x 14 +6x 2 +4 then the smallest good evaluation point is n = 6615, and |f (n)| \\u2248 1.3 \\u00d7 10 61 .\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Irreducibility Certificates for Z[x] via Evaluation\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"Here we present a much more practical way of certifying irreducibility by evaluation: we require just a sufficiently large prime factor. Let f \\u2208 Z[x] be non-constant, and let \\u03c1 \\u2208 Q be a root bound for f : that is, for every \\u03b1 \\u2208 C such that f (\\u03b1) = 0 we have |\\u03b1| \\u2264 \\u03c1. We note that it is relatively easy to compute root bounds (e.g. see [2] ). The following proposition was partly inspired by Theorem 2 in [3] , but appears to be new.\",\n",
      "            \"cite_spans\": [\n",
      "                {\n",
      "                    \"start\": 335,\n",
      "                    \"end\": 338,\n",
      "                    \"text\": \"[2]\",\n",
      "                    \"ref_id\": \"BIBREF1\"\n",
      "                },\n",
      "                {\n",
      "                    \"start\": 404,\n",
      "                    \"end\": 407,\n",
      "                    \"text\": \"[3]\",\n",
      "                    \"ref_id\": \"BIBREF3\"\n",
      "                }\n",
      "            ],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"A Large Prime Factor Suffices.\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"Proof. For a contradiction, suppose that f = gh \\u2208 Z[x] is a non-trivial factorization. We may assume that\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"A Large Prime Factor Suffices.\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"where d = deg f , C f \\u2208 Z is the leading coefficient, and the \\u03b1 j are the roots of f in C. We may assume that the \\u03b1 j are indexed so that the roots of g are \\u03b1 1 , . . . , \\u03b1 dg where d g = deg g.\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"A Large Prime Factor Suffices.\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"By evaluation we have f (n) = g(n) h(n) with all values in Z. Also f (n) = 0 since |n| > \\u03c1. We now estimate |g(n)|:\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"A Large Prime Factor Suffices.\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"where C g \\u2208 Z is the leading coefficient. Each factor in the product has magnitude greater than 1, so |g(n)| \\u2265 (|n|\\u2212\\u03c1) \\u0394 > s. Similarly, |h(n)| > s. This contradicts the given factorization f (n) = sp.\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"A Large Prime Factor Suffices.\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"When we have an evaluation point to which Proposition 1 applies we call it a large prime factor witness (abbr. LPFW) for f, \\u03c1 and \\u0394. We conjecture that every irreducible polynomial has infinitely many LPFWs; note that Bunyakowski's conjecture implies this.\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"A Large Prime Factor Suffices.\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"Example 3. This example shows that it can be beneficial to look for large prime factor witnesses rather than Bunyakowski prime witnesses.\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"A Large Prime Factor Suffices.\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"Let f = x 12 + 12x 4 + 92 and take \\u0394 = 1. We compute \\u03c1 = 7 4 as root bound, and then we obtain a LPFW at n = 5 with prime factor p = 81382739. In contrast, the smallest Bunyakowski prime is \\u2248 3.06 \\u00d7 10 41 at n = 2865.\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"A Large Prime Factor Suffices.\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"In the light of this example we exclude consideration of a certificate based on Bunyakowski's conjecture, and consider only LPFWs.\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"A Large Prime Factor Suffices.\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"We prefer to issue an LPFW certificate where the prime p is as small as \\\"reasonably possible\\\". Our implementation searches for suitable n in an incremental way, since smaller values of |n| produce smaller values of |f (n)|, and we expect smaller values of |f (n)| to be more likely to lead to an \\\"sp\\\" factorization with small prime factor p-this is only a heuristic, and does not guarantee to find the smallest such p. We look for the factorization |f (n)| = sp by trial division by the first few small primes (and GMP's probabilistic prime test for p).\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"A Large Prime Factor Suffices.\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"LPFW Certificate. An LPFW certificate comprises the following information:\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"A Large Prime Factor Suffices.\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"-a root bound \\u03c1, -a factor degree lower bound \\u0394 \\u2190\\u2212 with degree analysis certificate, -the evaluation point n > 1 + \\u03c1, -the large prime factor p of |f (n)| \\u2190 \\u2212 (opt.) with certificate of primality.\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"A Large Prime Factor Suffices.\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"Verification of an LPFW certificate entails:\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"A Large Prime Factor Suffices.\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"-evaluating f (n) and verifying that p is a factor; -verifying that the discarded factor s = |f (n)|/p satisfies s < (|n| \\u2212 \\u03c1) \\u0394 ; -verifying that \\u03c1 is a root bound for f \\u2190\\u2212 see comment below; -(if \\u0394 > 1) verifying that \\u0394 is a factor degree lower bound; -verifying that p is (probably) prime.\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"A Large Prime Factor Suffices.\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"In many cases the root bound can be verified simply by evaluation of a modified polynomial: let f (x) = d j=0 a j x j and set f * (x) = |a d |x d \\u2212 d\\u22121 j=0 |a j |x j , then if f * (\\u03c1) > 0 then \\u03c1 is a root bound for f . Some tighter root bounds may require applying an (iterated) Gr\\u00e4ffe transform to f first (e.g. see [2] ).\",\n",
      "            \"cite_spans\": [\n",
      "                {\n",
      "                    \"start\": 317,\n",
      "                    \"end\": 320,\n",
      "                    \"text\": \"[2]\",\n",
      "                    \"ref_id\": \"BIBREF1\"\n",
      "                }\n",
      "            ],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"A Large Prime Factor Suffices.\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"Example 4. This example shows how degree information can be useful in finding a small LPFW. Let f = x 4 \\u2212 1036x 2 + 7744. We find that \\u03c1 = 33 is a root bound. Without degree information (i.e. taking \\u0394 = 1) we obtain the first LPFW at n = 65 with corresponding prime p = 13481269. In contrast, from the factorization of f modulo 3 we can certify that \\u0394 = 2 is a factor degree lower bound for f . This information lets us obtain an LPFW at n = 47 with far smaller corresponding prime p = 14519.\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"A Large Prime Factor Suffices.\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"We define a (minor generalization of) a M\\u00f6bius transformation for Z[x]. The crucial property for us is that these transformations preserve irreducibility (except for some polynomials of degree 1).\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"M\\u00f6bius Transformations\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"In our applications the matrix entries will be integers, and we shall suppose that at least one of a and c is non-zero.\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"M\\u00f6bius Transformations\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"\\u03bc M is degenerate if det M = 0.\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Definition 2. A M\\u00f6bius transformation\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"Here is a summary of useful properties of a M\\u00f6bius transformation \\u03bc M . For part (f), suppose we have a counter-example f \\u2208 Z[x], then we have a non-trivial factorization \\u03bc M (f ) = gh, but by (b) and (d) we deduce that\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Definition 3. Let \\u03bc M be a M\\u00f6bius transform. We define the pseudo-inverse of \\u03bc M to be the M\\u00f6bius transformation corresponding to the classical adjoint\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"which is a non-trivial factorization, contradicting the assumption that f was irreducible.\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Definition 3. Let \\u03bc M be a M\\u00f6bius transform. We define the pseudo-inverse of \\u03bc M to be the M\\u00f6bius transformation corresponding to the classical adjoint\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"Our interest in M\\u00f6bius transformations is that they offer the possibility of finding a better LPFW certificate. Unfortunately we do not yet have a good way of determining which M\\u00f6bius transformations are helpful.\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Definition 3. Let \\u03bc M be a M\\u00f6bius transform. We define the pseudo-inverse of \\u03bc M to be the M\\u00f6bius transformation corresponding to the classical adjoint\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"Example 5. Let f = 97x 4 + 76x 3 + 78x 2 + 4x + 2. We obtain a LPFW certificate with \\u03c1 = 7/5, \\u0394 = 1, n = \\u22124 with corresponding prime factor p = 10601.\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Definition 3. Let \\u03bc M be a M\\u00f6bius transform. We define the pseudo-inverse of \\u03bc M to be the M\\u00f6bius transformation corresponding to the classical adjoint\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"Let M = 1 1 \\u22123 2 . Let g = prim(\\u03bc M (f )) = (x 4 + 1); by Proposition 2.(f) since deg g = deg f a LPFW certificate for g also certifies that f is irreducible. For g we obtain a certificate with \\u03c1 = 1, \\u0394 = 1, n = 2 with much smaller corresponding prime factor p = 17.\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Definition 3. Let \\u03bc M be a M\\u00f6bius transform. We define the pseudo-inverse of \\u03bc M to be the M\\u00f6bius transformation corresponding to the classical adjoint\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"Unsolved Problem: How to find a good M\\u00f6bius matrix M given just f ?\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Definition 3. Let \\u03bc M be a M\\u00f6bius transform. We define the pseudo-inverse of \\u03bc M to be the M\\u00f6bius transformation corresponding to the classical adjoint\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"Naturally, if we generate a LPFW certificate for a transformed polynomial \\u03bc M (f ) then we must indicate which M\\u00f6bius transformation was used. Given two polynomials f, g \\u2208 Z[x] of the same degree d, and M \\u2208 Mat 2\\u00d72 (Z), one can easily verify that g = prim(\\u03bc M (f )) by evaluating f at deg(f ) distinct rational points, and g at the (rational) transforms of these points, and then checking that the ratios of the values are all equal. So the extra information needed is M and \\u03bc M (f ).\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Certifying a Transformed Polynomial\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"Some content-free polynomials have non-trivial fixed divisors: an example is f = x 2 + x + 2 which is content-free but has fixed divisor 2.\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Fixed Divisors\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"Proof. The standard proof follows easily from representating of f with respect to the \\\"binomial basis\\\" for Z[x], namely { x k | k \\u2208 N}.\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Proposition 3. Let f \\u2208 Z[x] be non-zero. Its fixed divisor is equal to:\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"Polynomials having large fixed divisor \\u03b4 cannot have small LPFW certificates because we are forced to choose large evaluation points since we must have (|n| \\u2212 \\u03c1) \\u0394 > \\u03b4. This problem becomes more severe for higher degree polynomials since the fixed divisor can be as large as d! where d is the degree.\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Proposition 3. Let f \\u2208 Z[x] be non-zero. Its fixed divisor is equal to:\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"We can reduce the size of the fixed divisor by scaling the indeterminate (i.e. a M\\u00f6bius transformation for a diagonal matrix), or perhaps reversing the polynomial and scaling the indeterminate (i.e. a M\\u00f6bius transformation for an anti-diagonal matrix). We have not yet investigated the use of more general M\\u00f6bius transformations.\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Proposition 3. Let f \\u2208 Z[x] be non-zero. Its fixed divisor is equal to:\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"Let f \\u2208 Z[x] be content-free, irreducible with fixed divisor \\u03b4. Let q be a prime factor of \\u03b4, and let k be the multiplicity of q in |f (0)|. Then\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Proposition 3. Let f \\u2208 Z[x] be non-zero. Its fixed divisor is equal to:\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"has fixed divisor \\u03b4/q k . In practice, we consider several polynomials obtained by scaling x by q 1 , q 2 , . . . , q k ; in fact scaling by q \\u22121 , q \\u22122 , . . . can also be beneficial.\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Proposition 3. Let f \\u2208 Z[x] be non-zero. Its fixed divisor is equal to:\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"Our prototype implementation runs degree analysis and LPFW search \\\"in parallel\\\": i.e. it repeatedly alternates a few iterations of degree analysis with a few iterations of LPFW search. If degree analysis finds a new factor degree lower bound, \\u0394, this information is passed to the LPFW search.\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Implementation and Experimentation\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"We adopted the following strategy for choosing primes during degree analysis: initially we create a list of \\\"preferential primes\\\" (e.g. including the first few primes greater than the degree), then we pick primes alternately from this list or from a random generator. The range for randomly generated primes is gradually increased to favour finding quickly a certificate involving smaller primes (since these are computationally cheaper to verify). This strategy was inspired by some experimentation. There exist polynomials whose degree analysis certificates must involve \\\"large\\\" primes: e.g. a good set of primes for x 4 + 16x 3 + 5x 2 \\u2212 14x \\u2212 18 must contain at least one prime greater than 101. Also, empirically we find that a degree analysis certificate for an (even) Hermite polynomial must use primes greater than the degree.\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Degree Analysis\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"To issue a certificate, we look for a minimal cardinality subset of the primes used which suffices. This subset search is potentially exponential, but in our experiments it is very rare for a minimal subset to need more than 3 primes.\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Degree Analysis\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"As already mentioned, not all polynomials can be certified irreducible by degree analysis. A well-known class of polynomials for which irreducibility cannot be shown by degree analysis are the Swinnerton-Dyer polynomials: they are the minimal polynomials for sums of square-roots of \\\"independent\\\" integers. A more general class of such polynomials was presented in [6] .\",\n",
      "            \"cite_spans\": [\n",
      "                {\n",
      "                    \"start\": 365,\n",
      "                    \"end\": 368,\n",
      "                    \"text\": \"[6]\",\n",
      "                    \"ref_id\": \"BIBREF7\"\n",
      "                }\n",
      "            ],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Large Prime Factor Witness\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"We saw in Example 5, it can be better to issue a LPFW certificate for a transformed polynomial, but we do not yet have a good way of finding a good M\\u00f6bius transformation. Our current prototype implementation considers only indeterminate scaling and possibly reversal: i.e. the M\\u00f6bius matrix must be diagonal or anti-diagonal. A list of all scaling and reverse-scaling transforms by \\\"simple\\\" rationals is maintained, and the resulting polynomials are considered \\\"in parallel\\\". For each transformed polynomial we keep track of two evaluation points (one positive, one negative) and the corresponding evaluations. The evaluations are then considered in order of increasing absolute value; once an evaluation has been processed the corresponding evaluation point is incremented (or decremented, if it is negative).\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Large Prime Factor Witness\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"The LPFW search depends on a factor degree lower bound, \\u0394, which is initially 1. The degree analysis \\\"thread\\\" may at any time furnish a better value for \\u0394. So that this asynchrony can work well the LPFW search records, for each possible factor degree lower bound, any certificates it finds. When a higher \\u0394 is received, the search first checks whether a corresponding LPFW certificate has already been recorded; if so, that certificate is produced as output. Otherwise searching proceeds using the new \\u0394.\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Large Prime Factor Witness\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"Here are a few examples as computed by the current prototype, since degree analysis picks primes in a pseudo-random order different certificates may be issued for the same polynomial. A quick comment about run-times: our interpreted prototype favours producing certificates which are cheap to verify (rather than cheap to generate); the degree analysis certificates took \\u223c 0.25 s each to generate, the others \\u223c 0.5 s each. We did not measure verification run-time, but fully expect it to be less than 0.01 s in each case. In comparison, the polynomial factorizer in CoCoA took less than 0.01 s for all of these polynomials.\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Examples\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"As a larger example: the prototype took \\u223c 20 s (we expect the final implementation to be significantly faster) to produce a certificate for the degree 64 (Swinnerton-Dyer) minimal polynomial of \\u221a 61 + \\u221a 79 + \\u221a 139 + \\u221a 181 + \\u221a 199 + \\u221a 211\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Examples\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"This polynomial has fixed divisor \\u03b4 = 2 29 5 14 13 4 \\u2248 1.2 \\u00d7 10 28 . Our prototype found and applied the transformation x \\u2192 52 15 x, then produced an LPFW certificate for the transformed polynomial: \\u03c1 = 451/16, \\u0394 = 2 (with L = [19]), n = 46 and p \\u2248 7.5 \\u00d7 10 180 which was confirmed to be \\\"probably prime\\\" (according to GMP [5] ). The classical Berlekamp-Zassenhaus factorizer in CoCoA [1] took about 300 s to recognize irreducibility.\",\n",
      "            \"cite_spans\": [\n",
      "                {\n",
      "                    \"start\": 323,\n",
      "                    \"end\": 326,\n",
      "                    \"text\": \"[5]\",\n",
      "                    \"ref_id\": \"BIBREF6\"\n",
      "                },\n",
      "                {\n",
      "                    \"start\": 385,\n",
      "                    \"end\": 388,\n",
      "                    \"text\": \"[1]\",\n",
      "                    \"ref_id\": \"BIBREF0\"\n",
      "                }\n",
      "            ],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Examples\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"An anonymous referee reasonably asked about expected run-time or a (possibly heuristic) complexity analysis. The answer is \\\"It depends . . . \\\". For \\\"almost all\\\" polynomials, degree analysis suffices and is quick. In our setting, the LPFW search effectively happens only if a degree analysis certificate cannot be quickly found. In our experiments, the number of iterations in LPFW search before producing a certificate was quite irregular.\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"A Comment About Run-Time\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"As mentioned in the introduction there are many different criterions for certifying the irreducibility of a polynomial in Z[x]. Here we have concentrated on just two of them, and have pointed out how they can \\\"collaborate\\\".\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Conclusion\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"We have built a prototype implementation in CoCoA [1] , and plan to integrate it into CoCoALib, the underlying C++ library (where we expect significant performance gains).\",\n",
      "            \"cite_spans\": [\n",
      "                {\n",
      "                    \"start\": 50,\n",
      "                    \"end\": 53,\n",
      "                    \"text\": \"[1]\",\n",
      "                    \"ref_id\": \"BIBREF0\"\n",
      "                }\n",
      "            ],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Conclusion\"\n",
      "        },\n",
      "        {\n",
      "            \"text\": \"An interesting future possibility is for the requester of the certificate to state which criterions may be used (dictated by the implemented verifiers that the requester has available). But, a too restrictive choice of criterions may make it impossible to generate a certificate: e.g. there is no \\\"Eisenstein\\\" certificate for most polynomials.\",\n",
      "            \"cite_spans\": [],\n",
      "            \"ref_spans\": [],\n",
      "            \"section\": \"Conclusion\"\n",
      "        }\n",
      "    ],\n",
      "    \"bib_entries\": {\n",
      "        \"BIBREF0\": {\n",
      "            \"ref_id\": \"b0\",\n",
      "            \"title\": \"CoCoA: a system for doing Computations in Commutative Algebra\",\n",
      "            \"authors\": [\n",
      "                {\n",
      "                    \"first\": \"J\",\n",
      "                    \"middle\": [],\n",
      "                    \"last\": \"Abbott\",\n",
      "                    \"suffix\": \"\"\n",
      "                },\n",
      "                {\n",
      "                    \"first\": \"A\",\n",
      "                    \"middle\": [\n",
      "                        \"M\"\n",
      "                    ],\n",
      "                    \"last\": \"Bigatti\",\n",
      "                    \"suffix\": \"\"\n",
      "                },\n",
      "                {\n",
      "                    \"first\": \"L\",\n",
      "                    \"middle\": [],\n",
      "                    \"last\": \"Robbiano\",\n",
      "                    \"suffix\": \"\"\n",
      "                }\n",
      "            ],\n",
      "            \"year\": null,\n",
      "            \"venue\": \"\",\n",
      "            \"volume\": \"\",\n",
      "            \"issn\": \"\",\n",
      "            \"pages\": \"\",\n",
      "            \"other_ids\": {}\n",
      "        },\n",
      "        \"BIBREF1\": {\n",
      "            \"ref_id\": \"b1\",\n",
      "            \"title\": \"Bounds on factors in Z\",\n",
      "            \"authors\": [\n",
      "                {\n",
      "                    \"first\": \"J\",\n",
      "                    \"middle\": [],\n",
      "                    \"last\": \"Abbott\",\n",
      "                    \"suffix\": \"\"\n",
      "                }\n",
      "            ],\n",
      "            \"year\": null,\n",
      "            \"venue\": \"\",\n",
      "            \"volume\": \"\",\n",
      "            \"issn\": \"\",\n",
      "            \"pages\": \"\",\n",
      "            \"other_ids\": {}\n",
      "        },\n",
      "        \"BIBREF3\": {\n",
      "            \"ref_id\": \"b3\",\n",
      "            \"title\": \"Heugcd: how elementary upperbounds generate cheaper data\",\n",
      "            \"authors\": [\n",
      "                {\n",
      "                    \"first\": \"J\",\n",
      "                    \"middle\": [],\n",
      "                    \"last\": \"Davenport\",\n",
      "                    \"suffix\": \"\"\n",
      "                },\n",
      "                {\n",
      "                    \"first\": \"J\",\n",
      "                    \"middle\": [],\n",
      "                    \"last\": \"Padget\",\n",
      "                    \"suffix\": \"\"\n",
      "                }\n",
      "            ],\n",
      "            \"year\": null,\n",
      "            \"venue\": \"EUROCAL 1985\",\n",
      "            \"volume\": \"204\",\n",
      "            \"issn\": \"\",\n",
      "            \"pages\": \"18--28\",\n",
      "            \"other_ids\": {}\n",
      "        },\n",
      "        \"BIBREF5\": {\n",
      "            \"ref_id\": \"b5\",\n",
      "            \"title\": \"Sur quelques cas d'irr\\u00e9ductibilit\\u00e9 des polynomes\\u00e0 coefficients rationnels\",\n",
      "            \"authors\": [\n",
      "                {\n",
      "                    \"first\": \"G\",\n",
      "                    \"middle\": [],\n",
      "                    \"last\": \"Dumas\",\n",
      "                    \"suffix\": \"\"\n",
      "                }\n",
      "            ],\n",
      "            \"year\": 1906,\n",
      "            \"venue\": \"Journ. de Math\",\n",
      "            \"volume\": \"6\",\n",
      "            \"issn\": \"2\",\n",
      "            \"pages\": \"191--258\",\n",
      "            \"other_ids\": {}\n",
      "        },\n",
      "        \"BIBREF6\": {\n",
      "            \"ref_id\": \"b6\",\n",
      "            \"title\": \"GNU multiprecision library\",\n",
      "            \"authors\": [\n",
      "                {\n",
      "                    \"first\": \"T\",\n",
      "                    \"middle\": [],\n",
      "                    \"last\": \"Granlund\",\n",
      "                    \"suffix\": \"\"\n",
      "                }\n",
      "            ],\n",
      "            \"year\": null,\n",
      "            \"venue\": \"\",\n",
      "            \"volume\": \"\",\n",
      "            \"issn\": \"\",\n",
      "            \"pages\": \"\",\n",
      "            \"other_ids\": {}\n",
      "        },\n",
      "        \"BIBREF7\": {\n",
      "            \"ref_id\": \"b7\",\n",
      "            \"title\": \"A generalized class of polynomials that are hard to factor\",\n",
      "            \"authors\": [\n",
      "                {\n",
      "                    \"first\": \"E\",\n",
      "                    \"middle\": [],\n",
      "                    \"last\": \"Kaltofen\",\n",
      "                    \"suffix\": \"\"\n",
      "                },\n",
      "                {\n",
      "                    \"first\": \"D\",\n",
      "                    \"middle\": [\n",
      "                        \"R\"\n",
      "                    ],\n",
      "                    \"last\": \"Musser\",\n",
      "                    \"suffix\": \"\"\n",
      "                },\n",
      "                {\n",
      "                    \"first\": \"B\",\n",
      "                    \"middle\": [\n",
      "                        \"D\"\n",
      "                    ],\n",
      "                    \"last\": \"Saunders\",\n",
      "                    \"suffix\": \"\"\n",
      "                }\n",
      "            ],\n",
      "            \"year\": 1983,\n",
      "            \"venue\": \"SIAM J. Comput\",\n",
      "            \"volume\": \"12\",\n",
      "            \"issn\": \"\",\n",
      "            \"pages\": \"473--483\",\n",
      "            \"other_ids\": {}\n",
      "        },\n",
      "        \"BIBREF8\": {\n",
      "            \"ref_id\": \"b8\",\n",
      "            \"title\": \"Algebra, 3rd edn\",\n",
      "            \"authors\": [\n",
      "                {\n",
      "                    \"first\": \"S\",\n",
      "                    \"middle\": [],\n",
      "                    \"last\": \"Lang\",\n",
      "                    \"suffix\": \"\"\n",
      "                }\n",
      "            ],\n",
      "            \"year\": 1993,\n",
      "            \"venue\": \"\",\n",
      "            \"volume\": \"\",\n",
      "            \"issn\": \"\",\n",
      "            \"pages\": \"\",\n",
      "            \"other_ids\": {}\n",
      "        },\n",
      "        \"BIBREF9\": {\n",
      "            \"ref_id\": \"b9\",\n",
      "            \"title\": \"Neue Kriterien f\\u00fcr die Irreduzibilit\\u00e4t algebraischer Gleichungen\",\n",
      "            \"authors\": [\n",
      "                {\n",
      "                    \"first\": \"O\",\n",
      "                    \"middle\": [],\n",
      "                    \"last\": \"Perron\",\n",
      "                    \"suffix\": \"\"\n",
      "                }\n",
      "            ],\n",
      "            \"year\": 1907,\n",
      "            \"venue\": \"J. Reine Angew. Math\",\n",
      "            \"volume\": \"132\",\n",
      "            \"issn\": \"\",\n",
      "            \"pages\": \"288--307\",\n",
      "            \"other_ids\": {}\n",
      "        },\n",
      "        \"BIBREF10\": {\n",
      "            \"ref_id\": \"b10\",\n",
      "            \"title\": \"Every prime has a succinct certificate\",\n",
      "            \"authors\": [\n",
      "                {\n",
      "                    \"first\": \"V\",\n",
      "                    \"middle\": [\n",
      "                        \"R\"\n",
      "                    ],\n",
      "                    \"last\": \"Pratt\",\n",
      "                    \"suffix\": \"\"\n",
      "                }\n",
      "            ],\n",
      "            \"year\": 1975,\n",
      "            \"venue\": \"SIAM J. Comput\",\n",
      "            \"volume\": \"4\",\n",
      "            \"issn\": \"\",\n",
      "            \"pages\": \"214--220\",\n",
      "            \"other_ids\": {}\n",
      "        },\n",
      "        \"BIBREF11\": {\n",
      "            \"ref_id\": \"b11\",\n",
      "            \"title\": \"New proofs for two theorems of Capelli\",\n",
      "            \"authors\": [\n",
      "                {\n",
      "                    \"first\": \"E\",\n",
      "                    \"middle\": [],\n",
      "                    \"last\": \"Rowlinson\",\n",
      "                    \"suffix\": \"\"\n",
      "                }\n",
      "            ],\n",
      "            \"year\": 1964,\n",
      "            \"venue\": \"Can. Math. Bull\",\n",
      "            \"volume\": \"7\",\n",
      "            \"issn\": \"\",\n",
      "            \"pages\": \"431--433\",\n",
      "            \"other_ids\": {}\n",
      "        }\n",
      "    },\n",
      "    \"ref_entries\": {\n",
      "        \"FIGREF0\": {\n",
      "            \"text\": \"is irreducible and deg(\\u03bc M (f )) = deg(f ) then prim(\\u03bc M (f )) is irreducible. Proof. Parts (a) and (b) are elementary algebra. Part (c) follows from (a) and (b) by considering the factorization of f over a splitting field. Parts (d) and (e) are elementary for linear f ; the general case follows by repeated application of part (b).\",\n",
      "            \"latex\": null,\n",
      "            \"type\": \"figure\"\n",
      "        },\n",
      "        \"FIGREF1\": {\n",
      "            \"text\": \"x 16 + 4x 14 + 6x 2 + 4: degree analysis with prime list L = [13, 127] -x 4 + 16x 3 + 5x 2 \\u2212 14x \\u2212 18: degree analysis with prime list L = [107] -21-st cyclotomic polynomial: LPFW with \\u03c1 = 2, \\u0394 = 1, n = 3, and prime factor p = 368089 -Swinnerton-Dyer polynomial for [71, 113, 163]: LPFW with \\u03c1 = 43, \\u0394 = 2 (with L = [3]), n = 82 and prime factor p = 2367715751029 -97x 4 + 76x 3 + 78x 2 + 4x + 2: transform x \\u2192 2 x , LPFW \\u03c1 = 67/5, \\u0394 = 2 (with L = [3]), n = \\u221229 and prime factor p = 3041\",\n",
      "            \"latex\": null,\n",
      "            \"type\": \"figure\"\n",
      "        }\n",
      "    },\n",
      "    \"back_matter\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# get the file path for pdf.json\n",
    "all_json_path = glob.glob('./CovidData/document_parses/pdf_json/*.json')\n",
    "# check the structure of json file\n",
    "with open(all_json_path[1]) as f:\n",
    "    first_entry = json.load(f)\n",
    "    print(json.dumps(first_entry, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3b5388",
   "metadata": {},
   "source": [
    "The body text is the combination of all the `text`. Some of them might be blank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13129621",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### The following code is commented because after it takes long to generate dict,\n",
    "##### so we store dict in pickle file which can be loaded efficiently\n",
    "# write a class to read information from json files\n",
    "# class filereader:\n",
    "#     def __init__(self, file_path):\n",
    "#         with open(file_path) as f:\n",
    "#             content = json.load(f)\n",
    "#             self.paper_id = content['paper_id']\n",
    "            \n",
    "#             self.abstract = []\n",
    "#             self.body_text = []\n",
    "            \n",
    "#             # get abstract if the abstract is not none\n",
    "#             for entry in content['abstract']:\n",
    "#                 self.abstract.append(entry['text'])\n",
    "                \n",
    "            \n",
    "#             # get body text\n",
    "#             for entry in content['body_text']:\n",
    "#                 self.body_text.append(entry['text'])\n",
    "            \n",
    "#             # join abstract into one string\n",
    "#             self.abstract = '\\n'.join(self.abstract)\n",
    "#             self.body_text = '\\n'.join(self.body_text)\n",
    "\n",
    "# load json data into dataframe\n",
    "# json_dict = {'paper_id': [], 'abstract': [], 'body_text': []}\n",
    "# for idx, file_path in enumerate(all_json_path):\n",
    "#     if idx % (len(all_json_path) // 10) == 0:\n",
    "#         print('Processing index: {} of {}'.format(idx, len(all_json_path)))\n",
    "#     content = filereader(file_path)\n",
    "#     json_dict['paper_id'].append(content.paper_id)\n",
    "#     json_dict['abstract'].append(content.abstract)\n",
    "#     json_dict['body_text'].append(content.body_text)\n",
    "\n",
    "# with open('json_dict.pkl', 'wb') as f:\n",
    "#     pickle.dump(json_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ea5b72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read pmc_json files\n",
    "all_pmc_path = glob.glob('./CovidData/document_parses/pmc_json/*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f035d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a class to read information from pmc json files\n",
    "# It contains no abstracts\n",
    "# class filereader:\n",
    "#     def __init__(self, file_path):\n",
    "#         with open(file_path) as f:\n",
    "#             content = json.load(f)\n",
    "#             self.paper_id = content['paper_id']\n",
    "            \n",
    "#             self.body_text = []\n",
    "                  \n",
    "#             # get body text\n",
    "#             for entry in content['body_text']:\n",
    "#                 self.body_text.append(entry['text'])\n",
    "            \n",
    "#             # join abstract into one string\n",
    "#             self.body_text = '\\n'.join(self.body_text)\n",
    "\n",
    "# pmc_dict = {'paper_id': [], 'body_text': []}\n",
    "# for idx, file_path in enumerate(all_pmc_path):\n",
    "#     if idx % (len(all_pmc_path) // 10) == 0:\n",
    "#         print('Processing index: {} of {}'.format(idx, len(all_pmc_path)))\n",
    "#     content = filereader(file_path)\n",
    "#     pmc_dict['paper_id'].append(content.paper_id)\n",
    "#     pmc_dict['body_text'].append(content.body_text)\n",
    "\n",
    "# with open('pmc_dict.pkl', 'wb') as f:\n",
    "#     pickle.dump(pmc_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7152c3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read pickle file of pdf.json\n",
    "json_dict_pickle = pickle.load(open('json_dict.pkl', 'rb'))\n",
    "json_df = pd.DataFrame(json_dict_pickle, columns=['paper_id', 'abstract', 'body_text'])\n",
    "\n",
    "# read pickle file of pmc.json\n",
    "pmc_dict_pickle = pickle.load(open('pmc_dict.pkl', 'rb'))\n",
    "pmc_df = pd.DataFrame(pmc_dict_pickle, columns=['paper_id', 'body_text'])\n",
    "# remove blank body text\n",
    "pmc_df = pmc_df[pmc_df.body_text != ' ']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8938a48",
   "metadata": {},
   "source": [
    "### Merge the pdf & pmc data with the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0936c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge pdf with metadata on sha\n",
    "data = pd.merge(json_df, metadata, left_on='paper_id', right_on='sha', how='left').drop('sha', axis=1)\n",
    "# merge pmc with data\n",
    "data = pd.merge(data, pmc_df, left_on='pmcid', right_on='paper_id', how='left').drop('paper_id_y', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701e5b69",
   "metadata": {},
   "source": [
    "### Data cleaning\n",
    "* Remove rows with duplicated body_text, paper_id\n",
    "* Keep the abstract (abstract_x) in metadata, impute the null with the abstract (abstract_y) from pdf file\n",
    "* Keep the body text (body_text_y) from pmc files, impute the null with the body text (body_text_x) from pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0c2f6804",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "Index(['paper_id'], dtype='object')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-52bc12fd6305>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'body_text_x'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body_text_y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# remove rows with duplicated paper_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'paper_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/gscratch/stf/nelljy/tools/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop_duplicates\u001b[0;34m(self, subset, keep, inplace, ignore_index)\u001b[0m\n\u001b[1;32m   5269\u001b[0m         \u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_bool_kwarg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"inplace\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5270\u001b[0m         \u001b[0mignore_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_bool_kwarg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ignore_index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5271\u001b[0;31m         \u001b[0mduplicated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5273\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gscratch/stf/nelljy/tools/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mduplicated\u001b[0;34m(self, subset, keep)\u001b[0m\n\u001b[1;32m   5403\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5405\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5407\u001b[0m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: Index(['paper_id'], dtype='object')"
     ]
    }
   ],
   "source": [
    "# remove rows with duplicated body_text\n",
    "data.drop_duplicates(['body_text_x', 'body_text_y'], inplace=True)\n",
    "# remove rows with duplicated paper_id\n",
    "data.drop_duplicates(['paper_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f69cdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstract_x is from json files (missing is blank), abstract_y is from metadata (missing is null)\n",
    "# a large proportion of data having different abstract\n",
    "print('The proportion of data having different abstracts:', \n",
    "      data[data.abstract_x != data.abstract_y].shape[0] / data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e158ccfd",
   "metadata": {},
   "source": [
    "Remark: many of the differences are slight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d106875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[data.abstract_x != data.abstract_y][['abstract_x', 'abstract_y', 'url']].tail(3)\n",
    "# data[(data.abstract_x != data.abstract_y) &\n",
    "#      (data.abstract_y.isnull())][['abstract_x', 'abstract_y', 'url', 'body_text_x', 'body_text_y']]\n",
    "# For e.g., the abstract in json file is not correct, while the abstract in metadata is correct\n",
    "# the abstract in metadata seems to be more reliable, so we use these, and fill the missing values in abstract_y \n",
    "# with the abstract_x\n",
    "data.loc[data.abstract_y.isnull() & \n",
    "         (data.abstract_x != '') & \n",
    "         (data.abstract_x != ' ') & \n",
    "         (~data.abstract_x.isnull()), 'abstract_y'] = data[data.abstract_y.isnull() &\n",
    "                                                           (data.abstract_x != '') & \n",
    "                                                           (data.abstract_x != ' ') & \n",
    "                                                           (~data.abstract_x.isnull())].abstract_x\n",
    "data.rename(columns={'abstract_y': 'abstract'}, inplace=True)\n",
    "data.drop('abstract_x', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49ab83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# body_text_x is from json files, body_text_y is from pmc files\n",
    "# most of the data have different body text\n",
    "print('The proportion of data having different body text:',\n",
    "      data[data.body_text_x != data.body_text_y].shape[0] / data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9870de7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar body_text\n",
    "# data[data.body_text_x != data.body_text_y][['body_text_x', 'body_text_y', 'url']]\n",
    "# use the text from pmc file (body_text_y) trusting the statement that it's of higher quality\n",
    "# print(data.body_text_x.isnull().sum(), data[data.body_text_x == ''].shape, data[data.body_text_x == ' '].shape)\n",
    "# print(data.body_text_y.isnull().sum(), data[data.body_text_y == ''].shape, data[data.body_text_y == ' '].shape)\n",
    "data.loc[data.body_text_y.isnull(), 'body_text_y'] = data[data.body_text_y.isnull()].body_text_x\n",
    "data.rename(columns = {'body_text_y': 'body_text'}, inplace=True)\n",
    "data.drop('body_text_x', axis=1, inplace=True)\n",
    "data.rename(columns={'paper_id_x': 'paper_id', 'source_x': 'source'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5c1289",
   "metadata": {},
   "source": [
    "### Create new features\n",
    "New features are:\n",
    "* is_covid19: 0 indicates that the publication is not related to covid19, 1 for otherwise\n",
    "* text_language: the language label of the publication (remove non-english publications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eaf8952f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column for covid19\n",
    "data['is_covid19'] = data.body_text.str.contains('COVID-19|covid|sar cov 2|SARS-CoV-2|2019-nCov|2019 ncov|SARS Coronavirus 2|2019 Novel Coronavirus|coronavirus 2019| Wuhan coronavirus|wuhan pneumonia|wuhan virus', case=False)\n",
    "print('The percentage of covid19 related publications:', data.is_covid19.sum()/data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c5327825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy_langdetect.spacy_langdetect.LanguageDetector at 0x2b5e0c070130>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use nlp to detect language: we want to remove the non-english publications for efficiency.\n",
    "nlp = en_core_sci_lg.load(disable=['tagger', 'ner', 'lemmatizer'])\n",
    "nlp.add_pipe('language_detector', last=True)\n",
    "# use the first 500 words to determine the language it uses\n",
    "# data['text_language'] = data.body_text.apply(lambda x: nlp(x[:500])._.language['language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0e5a2129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dict = {'data': data}\n",
    "# with open('data_dict.pkl', 'wb') as f:\n",
    "#     pickle.dump(data_dict, f)\n",
    "data = pickle.load(open('data_dict.pkl', 'rb'))['data']\n",
    "data = data[data.text_language == 'en']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c19c90",
   "metadata": {},
   "source": [
    "Save to csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3ed97fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('covid19pub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c51130",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
